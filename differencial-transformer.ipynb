{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Differential Transformer"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:14:17.240625Z","iopub.status.busy":"2025-01-28T17:14:17.240442Z","iopub.status.idle":"2025-01-28T17:14:29.392442Z","shell.execute_reply":"2025-01-28T17:14:29.391417Z","shell.execute_reply.started":"2025-01-28T17:14:17.240606Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers==4.48.1"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:14:29.393714Z","iopub.status.busy":"2025-01-28T17:14:29.393432Z","iopub.status.idle":"2025-01-28T17:14:48.792961Z","shell.execute_reply":"2025-01-28T17:14:48.792283Z","shell.execute_reply.started":"2025-01-28T17:14:29.393692Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import LlamaConfig\n","from typing import Callable, List, Optional, Tuple, Union\n","from transformers.cache_utils import Cache\n","import math\n","from transformers.models.llama.modeling_llama import LlamaAttention, LlamaModel, LlamaForCausalLM\n","\n","\n","from transformers import LlamaPreTrainedModel\n","from typing import Optional, Tuple, Union\n","import torch\n","from torch import nn\n","import os\n","from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm, LlamaRotaryEmbedding\n","import tempfile\n","from pathlib import Path\n","import safetensors.torch as sfts\n","from huggingface_hub import hf_hub_download, snapshot_download, HfApi"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:15:37.641529Z","iopub.status.busy":"2025-01-28T17:15:37.641220Z","iopub.status.idle":"2025-01-28T17:15:37.649321Z","shell.execute_reply":"2025-01-28T17:15:37.648474Z","shell.execute_reply.started":"2025-01-28T17:15:37.641508Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing modeling.py\n"]}],"source":["%%writefile modeling.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import LlamaConfig\n","from typing import Callable, List, Optional, Tuple, Union\n","from transformers.cache_utils import Cache\n","import math\n","from transformers.models.llama.modeling_llama import LlamaAttention, LlamaModel, LlamaForCausalLM\n","\n","\n","from transformers import LlamaPreTrainedModel\n","from typing import Optional, Tuple, Union\n","import torch\n","from torch import nn\n","import os\n","from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm, LlamaRotaryEmbedding\n","import tempfile\n","from pathlib import Path\n","import safetensors.torch as sfts\n","from huggingface_hub import hf_hub_download, snapshot_download, HfApi\n","\n","def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n","    \"\"\"torch.repeat_interleave(x, dim=1, repeats=n_rep)\"\"\"\n","    bs, n_kv_heads, slen, head_dim = x.shape\n","    if n_rep == 1:\n","        return x\n","    return (\n","        x[:, :, None, :, :]\n","        .expand(bs, n_kv_heads, n_rep, slen, head_dim)\n","        .reshape(bs, n_kv_heads * n_rep, slen, head_dim)\n","    )\n","\n","def lambda_init_fn(depth):\n","    return 0.8 - 0.6 * math.exp(-0.3 * depth)\n","\n","class RMSNorm(nn.Module):\n","    def __init__(self, dim: int, eps: float = 1e-6, elementwise_affine=True, memory_efficient=False):\n","        super().__init__()\n","        self.dim = dim\n","        self.eps = eps\n","        self.elementwise_affine = elementwise_affine\n","        if self.elementwise_affine:\n","            self.weight = nn.Parameter(torch.ones(dim))\n","        else:\n","            self.register_parameter('weight', None)\n","\n","    def _norm(self, x):\n","        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n","\n","    def forward(self, x):\n","        output = self._norm(x.float()).type_as(x)\n","        if self.weight is not None:\n","            output = output * self.weight\n","        return output\n","\n","    def extra_repr(self) -> str:\n","        return f'dim={self.dim}, eps={self.eps}, elementwise_affine={self.elementwise_affine}'\n","\n","def rotate_half(x):\n","    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n","    x1 = x[..., : x.shape[-1] // 2]\n","    x2 = x[..., x.shape[-1] // 2 :]\n","    return torch.cat((-x2, x1), dim=-1)\n","\n","def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n","    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n","\n","    Args:\n","        q (`torch.Tensor`): The query tensor.\n","        k (`torch.Tensor`): The key tensor.\n","        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n","        sin (`torch.Tensor`): The sine part of the rotary embedding.\n","        position_ids (`torch.Tensor`, *optional*):\n","            Deprecated and unused.\n","        unsqueeze_dim (`int`, *optional*, defaults to 1):\n","            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n","            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n","            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n","            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n","            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n","            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n","    Returns:\n","        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n","    \"\"\"\n","    cos = cos.unsqueeze(unsqueeze_dim)\n","    sin = sin.unsqueeze(unsqueeze_dim)\n","    q_embed = (q * cos) + (rotate_half(q) * sin)\n","    k_embed = (k * cos) + (rotate_half(k) * sin)\n","    return q_embed, k_embed\n","\n","\n","class LlamaDiffAttention(nn.Module):\n","    \"\"\"Multi-headed attention with DiffAttention technique.\"\"\"\n","\n","    def __init__(self, config: LlamaConfig, layer_idx: int):\n","        super().__init__()\n","        self.config = config\n","        self.layer_idx = layer_idx\n","        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n","        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n","        self.scaling = self.head_dim**-0.5\n","        self.attention_dropout = config.attention_dropout\n","        self.is_causal = True\n","\n","        # Keep original projection sizes\n","        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias)\n","        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n","        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n","        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias)\n","\n","        # DiffAttention-specific parameters\n","        self.lambda_init = lambda_init_fn(layer_idx)\n","        self.lambda_q1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n","        self.lambda_k1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n","        self.lambda_q2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n","        self.lambda_k2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0, std=0.1))\n","\n","        self.subln = RMSNorm(self.head_dim, eps=1e-5, elementwise_affine=True)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        past_key_value: Optional[Cache] = None,\n","        cache_position: Optional[torch.LongTensor] = None,\n","        **kwargs,\n","    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n","        input_shape = hidden_states.shape[:-1]\n","        batch_size, seq_length = input_shape\n","        hidden_shape = (batch_size, seq_length, -1, self.head_dim)\n","\n","        # Project to get query and key states (keeping original dimensions)\n","        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n","        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n","        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n","\n","        # Apply rotary embeddings if provided\n","        if position_embeddings is not None:\n","            cos, sin = position_embeddings\n","            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n","\n","        # Handle past key values if provided\n","        if past_key_value is not None:\n","            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n","            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n","\n","        # Repeat for grouped query attention\n","        key_states = repeat_kv(key_states, self.num_key_value_groups)\n","        value_states = repeat_kv(value_states, self.num_key_value_groups)\n","\n","        # Scale queries\n","        query_states *= self.scaling\n","\n","        # Create two versions of attention weights using the same projections\n","        attn_weights_1 = torch.matmul(query_states, key_states.transpose(-1, -2))\n","        # For the second attention, we apply a learned transformation to query and key\n","        query_states_2 = query_states * self.lambda_q2.unsqueeze(0).unsqueeze(0)\n","        key_states_2 = key_states * self.lambda_k2.unsqueeze(0).unsqueeze(0)\n","        attn_weights_2 = torch.matmul(query_states_2, key_states_2.transpose(-1, -2))\n","\n","        if attention_mask is not None:\n","            attn_weights_1 += attention_mask\n","            attn_weights_2 += attention_mask\n","\n","        # Process attention weights\n","        attn_weights_1 = torch.nan_to_num(attn_weights_1)\n","        attn_weights_2 = torch.nan_to_num(attn_weights_2)\n","        \n","        attn_weights_1 = F.softmax(attn_weights_1, dim=-1, dtype=torch.float32).type_as(attn_weights_1)\n","        attn_weights_2 = F.softmax(attn_weights_2, dim=-1, dtype=torch.float32).type_as(attn_weights_2)\n","\n","        # Compute differential lambdas\n","        lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float()).type_as(query_states)\n","        lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float()).type_as(query_states)\n","        lambda_full = lambda_1 - lambda_2 + self.lambda_init\n","\n","        # Combine attention weights with differential scaling\n","        attn_weights = attn_weights_1 - lambda_full * attn_weights_2\n","\n","        # Compute attention output\n","        attn_output = torch.matmul(attn_weights, value_states)\n","        attn_output = self.subln(attn_output)\n","        attn_output = attn_output * (1 - self.lambda_init)\n","        attn_output = attn_output.transpose(1, 2).reshape(*input_shape, -1).contiguous()\n","\n","        attn_output = self.o_proj(attn_output)\n","        return attn_output, attn_weights\n","\n","def replace_llama_attention_with_diffattention(model: nn.Module):\n","    \"\"\"\n","    Replace all instances of LlamaAttention in the model with LlamaDiffAttention,\n","    keeping the weights intact.\n","    \n","    Args:\n","        model (nn.Module): The model containing LlamaAttention layers.\n","    \n","    Returns:\n","        nn.Module: The updated model with LlamaDiffAttention.\n","    \"\"\"\n","    for name, module in model.named_modules():\n","        if isinstance(module, LlamaAttention):\n","            # Extract the configuration and layer index\n","            config = module.config\n","            layer_idx = module.layer_idx\n","\n","            # Create a new LlamaDiffAttention instance\n","            diff_attention = LlamaDiffAttention(config, layer_idx)\n","\n","            # Copy weights from the original LlamaAttention\n","            diff_attention.q_proj.weight.data = module.q_proj.weight.data.clone()\n","            diff_attention.k_proj.weight.data = module.k_proj.weight.data.clone()\n","            diff_attention.v_proj.weight.data = module.v_proj.weight.data.clone()\n","            diff_attention.o_proj.weight.data = module.o_proj.weight.data.clone()\n","            if config.attention_bias:\n","                diff_attention.q_proj.bias.data = module.q_proj.bias.data.clone()\n","                diff_attention.k_proj.bias.data = module.k_proj.bias.data.clone()\n","                diff_attention.v_proj.bias.data = module.v_proj.bias.data.clone()\n","                diff_attention.o_proj.bias.data = module.o_proj.bias.data.clone()\n","\n","            # Replace the module in the model\n","            parent_module = model\n","            for part in name.split('.')[:-1]:\n","                parent_module = getattr(parent_module, part)\n","            setattr(parent_module, name.split('.')[-1], diff_attention)\n","\n","    return model\n","\n","class DiffLlamaModel(LlamaModel):\n","    config_class = LlamaConfig\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.padding_idx = config.pad_token_id\n","        self.vocab_size = config.vocab_size\n","\n","        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n","        self.layers = nn.ModuleList(\n","            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n","        )\n","        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n","        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n","        self.gradient_checkpointing = False\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","        \n","        self.layers = replace_llama_attention_with_diffattention(self.layers)\n","        \n","    def push_to_hub(self, repo_id: str, token: str = None, private: bool = False):\n","        api = HfApi(token=token)\n","        repo_id = api.create_repo(repo_id=repo_id, exist_ok=True, private=private).repo_id\n","        print(\"Creating repository:\", repo_id)\n","        \n","        with tempfile.TemporaryDirectory() as tmp:\n","            # Create the full path for saving\n","            saved_path = Path(tmp) / repo_id\n","            saved_path.mkdir(parents=True, exist_ok=True)\n","            \n","            # Prepare the model file path\n","            model_path = saved_path / \"model.safetensors\"\n","            \n","            # Get and process state dict\n","            state_dict = self.state_dict()\n","            state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n","            \n","            print(f\"Saving model to {model_path}\")\n","            sfts.save_file(state_dict, str(model_path))\n","            \n","            print(\"Uploading to Hub...\")\n","            api.upload_folder(\n","                folder_path=str(saved_path),\n","                repo_id=repo_id,\n","                path_in_repo=\"\",\n","            )\n","            print(\"Upload complete!\")\n","\n","    \n","    @classmethod\n","    def from_llama_model(cls, model):\n","        base_model = model.model if hasattr(model, 'model') else model\n","        \n","        # Create a new DiffLlamaModel instance with the same config\n","        diff_model = cls(base_model.config)\n","        \n","        # Copy weights from the original model\n","        diff_model.embed_tokens.weight.data = base_model.embed_tokens.weight.data.clone()\n","        diff_model.norm.weight.data = base_model.norm.weight.data.clone()\n","        \n","        # Copy layer weights\n","        for i, (orig_layer, diff_layer) in enumerate(zip(base_model.layers, diff_model.layers)):\n","            # Copy non-attention weights\n","            diff_layer.mlp.gate_proj.weight.data = orig_layer.mlp.gate_proj.weight.data.clone()\n","            diff_layer.mlp.down_proj.weight.data = orig_layer.mlp.down_proj.weight.data.clone()\n","            diff_layer.mlp.up_proj.weight.data = orig_layer.mlp.up_proj.weight.data.clone()\n","            diff_layer.input_layernorm.weight.data = orig_layer.input_layernorm.weight.data.clone()\n","            diff_layer.post_attention_layernorm.weight.data = orig_layer.post_attention_layernorm.weight.data.clone()\n","            \n","            # The attention weights are handled by replace_llama_attention_with_diffattention\n","            # which was called in __init__\n","        \n","        return diff_model\n","\n","    \n","    @classmethod\n","    def from_pretrained(\n","            cls,\n","            pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n","            token = None,\n","            config = None,\n","            *model_args,\n","            **kwargs\n","        ):\n","        with tempfile.TemporaryDirectory() as tmp:\n","            # load the repo and the config\n","            snapshot_download(repo_id=pretrained_model_name_or_path, local_dir=f\"{tmp}/repo/\", token=token)\n","            if config is None:\n","                config = LlamaConfig.from_pretrained(f\"{tmp}/repo/\", token=token)\n","                \n","            model = replace_llama_attention_with_diffattention(cls(config))\n","            sfts.load_model(model, f\"{tmp}/repo/model.safetensors\")\n","            \n","        return model\n","\n","\n","class DiffLlamaForCausalLM(LlamaForCausalLM):\n","    tied_weights_keys = [\"lm_head.weight\"]\n","    tp_plan = {\"lm_head\": \"colwise_rep\"}\n","    \n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.model = DiffLlamaModel(config)\n","        self.vocab_size = config.vocab_size\n","        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n","        \n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def push_to_hub(self, repo_id: str, token: str = None, private: bool = False):\n","        api = HfApi(token=token)\n","        repo_id = api.create_repo(repo_id=repo_id, exist_ok=True, private=private).repo_id\n","        print(\"Creating repository:\", repo_id)\n","        \n","        with tempfile.TemporaryDirectory() as tmp:\n","            saved_path = Path(tmp) / repo_id\n","            saved_path.mkdir(parents=True, exist_ok=True)\n","            \n","            model_path = saved_path / \"model.safetensors\"\n","            \n","            state_dict = self.state_dict()\n","            print(f\"Saving model to {model_path}\")\n","            sfts.save_file(state_dict, str(model_path))\n","        \n","            api.upload_folder(\n","                folder_path=str(saved_path),\n","                repo_id=repo_id,\n","                path_in_repo=\"\",\n","            )\n","    \n","    @classmethod\n","    def from_llama_model(cls, model):\n","        diff_model = cls(model.config)\n","        diff_model.model = DiffLlamaModel.from_llama_model(model.model)\n","        diff_model.lm_head.weight.data = model.lm_head.weight.data.clone()\n","        return diff_model\n","    \n","    @classmethod\n","    def from_pretrained(\n","            cls,\n","            pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n","            token = None,\n","            config = None,\n","            *model_args,\n","            **kwargs\n","        ):\n","        with tempfile.TemporaryDirectory() as tmp:\n","            snapshot_download(repo_id=pretrained_model_name_or_path, local_dir=f\"{tmp}/repo/\", token=token)\n","            if config is None:\n","                config = LlamaConfig.from_pretrained(f\"{tmp}/repo/\", token=token)\n","            \n","            model = cls(config)\n","            \n","            # Load the state dict\n","            state_dict = sfts.load_file(f\"{tmp}/repo/model.safetensors\")\n","            \n","            # Load the processed state dict\n","            model.load_state_dict(state_dict)\n","            \n","        return model"]},{"cell_type":"markdown","metadata":{},"source":["## Train Diff"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:15:42.409586Z","iopub.status.busy":"2025-01-28T17:15:42.409278Z","iopub.status.idle":"2025-01-28T17:15:46.247712Z","shell.execute_reply":"2025-01-28T17:15:46.246881Z","shell.execute_reply.started":"2025-01-28T17:15:42.409560Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q accelerate --upgrade"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:15:46.249555Z","iopub.status.busy":"2025-01-28T17:15:46.249230Z","iopub.status.idle":"2025-01-28T17:16:01.329493Z","shell.execute_reply":"2025-01-28T17:16:01.328758Z","shell.execute_reply.started":"2025-01-28T17:15:46.249521Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marthur-lagacherie\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["!pip install -q wandb\n","\n","import wandb\n","wandb.login(key=\"\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:16:01.331575Z","iopub.status.busy":"2025-01-28T17:16:01.331061Z","iopub.status.idle":"2025-01-28T17:16:01.449828Z","shell.execute_reply":"2025-01-28T17:16:01.449124Z","shell.execute_reply.started":"2025-01-28T17:16:01.331551Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import login\n","\n","login(\"hf_xxx\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:16:04.761334Z","iopub.status.busy":"2025-01-28T17:16:04.761056Z","iopub.status.idle":"2025-01-28T17:16:04.767713Z","shell.execute_reply":"2025-01-28T17:16:04.766848Z","shell.execute_reply.started":"2025-01-28T17:16:04.761313Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing main.py\n"]}],"source":["%%writefile main.py\n","import torch\n","from accelerate import Accelerator\n","from tqdm.auto import tqdm\n","import wandb\n","import argparse\n","from dataclasses import dataclass\n","from typing import Optional, List\n","import yaml\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n","import numpy as np\n","from modeling import * \n","\n","@dataclass\n","class TrainingConfig:\n","    # Model configuration\n","    model_name: str = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n","    max_length: int = 2048\n","    \n","    # Dataset configuration\n","    dataset_name: str = \"Arthur-LAGACHERIE/EntierInstruct-66k\"\n","    dataset_split: str = \"train\"\n","    subset:str = \"all\"\n","    select: int = None\n","    question_column: str = \"question\"\n","    answer_column: str = \"answer\"\n","    val_size: int = 2000\n","    \n","    # Training configuration\n","    batch_size: int = 1\n","    num_epochs: int = 3\n","    learning_rate: float = 3e-4\n","    eval_steps: int = 100\n","    num_workers: int = 1\n","    \n","    # Logging configuration\n","    project_name: str = \"test\"\n","    run_name: Optional[str] = None\n","    \n","    @classmethod\n","    def from_yaml(cls, yaml_path: str) -> 'TrainingConfig':\n","        with open(yaml_path, 'r') as f:\n","            config_dict = yaml.safe_load(f)\n","        return cls(**config_dict)\n","    \n","    def save_yaml(self, yaml_path: str):\n","        with open(yaml_path, 'w') as f:\n","            yaml.dump(self.__dict__, f)\n","\n","def get_dataloaders(config: TrainingConfig):\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","    from datasets import load_dataset\n","    \"\"\"\n","    if config.select is not None:\n","        dataset = load_dataset(config.dataset_name, split=config.dataset_split)\n","        dataset = dataset.select(range(config.select))\n","    else:\n","        dataset = load_dataset(config.dataset_name, split=config.dataset_split)\n","    \n","    def apply_template(example):\n","        formatted_messages = []\n","        for message in example['messages']:\n","            role = message['role']\n","            content = message['content']\n","            if role == \"user\":\n","                formatted_messages.append(f\"<|im_start|>user\\n{content}<|im_end|>\")\n","            elif role == \"assistant\":\n","                formatted_messages.append(f\"<|im_start|>assistant\\n{content}<|im_end|>\")\n","        return {\"text\": \"\\n\".join(formatted_messages)}\n","        \n","    dataset = dataset.map(apply_template)\n","    \n","    def tokenization(example):\n","        return tokenizer(example[\"text\"], truncation=True, max_length=config.max_length)\n","    \n","    dataset = dataset.map(tokenization, batched=True)\n","    column_to_remove = [col for col in dataset.column_names if col not in [\"input_ids\", \"attention_mask\"]]\n","    dataset = dataset.remove_columns(column_to_remove)\n","    print(dataset)\n","    \"\"\"\n","    dataset = load_dataset(config.dataset_name, split=\"train\")\n","    print(dataset)\n","    train_dataset = dataset\n","    val_dataset = dataset.select(np.random.choice(len(dataset), config.val_size, replace=False))\n","    \n","    from transformers import DataCollatorForLanguageModeling\n","    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n","    \n","    from torch.utils.data import DataLoader\n","    train_dataloader = DataLoader(\n","        train_dataset, \n","        shuffle=True, \n","        batch_size=config.batch_size, \n","        collate_fn=data_collator, \n","        num_workers=config.num_workers\n","    )\n","    val_dataloader = DataLoader(\n","        val_dataset, \n","        shuffle=False, \n","        batch_size=config.batch_size, \n","        collate_fn=data_collator, \n","        num_workers=config.num_workers\n","    )\n","    return train_dataloader, val_dataloader\n","\n","def get_model(config: TrainingConfig):\n","    model = AutoModelForCausalLM.from_pretrained(config.model_name)\n","    return DiffLlamaForCausalLM.from_llama_model(model)\n","\n","\n","def train(config: TrainingConfig, model, train_dataloader, val_dataloader):\n","    accelerator = Accelerator()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n","    \n","    train_dataloader, val_dataloader, model, optimizer = accelerator.prepare(\n","        train_dataloader, val_dataloader, model, optimizer\n","    )\n","    \n","    run_name = config.run_name or f\"{config.project_name}-{config.model_name.split('/')[-1]}\"\n","    wandb.init(project=config.project_name, name=run_name, config=config.__dict__)\n","    \n","    model.train()\n","    total_steps = len(train_dataloader) * config.num_epochs\n","    \n","    def evaluate(model, val_dataloader):\n","        model.eval()\n","        total_loss = 0\n","        with torch.no_grad():\n","            for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n","                outputs = model(**batch)\n","                total_loss += outputs.loss.item()\n","        model.train()\n","        return total_loss / len(val_dataloader)\n","    \n","    with tqdm(desc=\"Training\", total=total_steps) as pbar:\n","        for epoch in range(config.num_epochs):\n","            for step, batch in enumerate(train_dataloader):\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                \n","                accelerator.backward(loss)\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                \n","                pbar.update(1)\n","                \n","                if step % config.eval_steps == 0:\n","                    val_loss = evaluate(model, val_dataloader)\n","                    wandb.log({\n","                        \"val_loss\": val_loss,\n","                        \"global_step\": step + epoch * len(train_dataloader),\n","                    })\n","    \n","    wandb.finish()\n","\n","def main():\n","    parser = argparse.ArgumentParser(description=\"Training script with Accelerate\")\n","    parser.add_argument(\"--config\", type=str, help=\"Path to YAML config file\")\n","    parser.add_argument(\"--model_name\", type=str, help=\"Model name or path\")\n","    parser.add_argument(\"--batch_size\", type=int, help=\"Batch size\")\n","    parser.add_argument(\"--num_epochs\", type=int, help=\"Number of training epochs\")\n","    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate\")\n","    parser.add_argument(\"--project_name\", type=str, help=\"WandB project name\")\n","    parser.add_argument(\"--run_name\", type=str, help=\"WandB run name\")\n","    \n","    args = parser.parse_args()\n","    \n","    if args.config:\n","        config = TrainingConfig.from_yaml(args.config)\n","    else:\n","        config = TrainingConfig()\n","    \n","    # Override config with command line arguments\n","    for arg in vars(args):\n","        if getattr(args, arg) is not None and arg != 'config':\n","            setattr(config, arg, getattr(args, arg))\n","    \n","    mloramodel = get_model(config)\n","    train_dataloader, val_dataloader = get_dataloaders(config)\n","    \n","    train(config, mloramodel, train_dataloader, val_dataloader)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:19:35.265906Z","iopub.status.busy":"2025-01-28T17:19:35.265573Z","iopub.status.idle":"2025-01-28T17:19:35.275517Z","shell.execute_reply":"2025-01-28T17:19:35.274675Z","shell.execute_reply.started":"2025-01-28T17:19:35.265881Z"},"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Optional, List\n","import yaml\n","\n","@dataclass\n","class TrainingConfig():\n","    # Model configuration\n","    model_name: str = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n","    max_length: int = 2048\n","    \n","    # Dataset configuration\n","    dataset_name: str = \"Arthur-LAGACHERIE/smoltalk-semhashed\"\n","    dataset_split: str = \"train\"\n","    subset:str = \"main\"\n","    select: int = None\n","    question_column: str = \"messages\"\n","    answer_column: str = \"messages\"\n","    val_size: int = 2000\n","    \n","    # Training configuration\n","    batch_size: int = 1\n","    num_epochs: int = 1\n","    learning_rate: float = 3e-4\n","    eval_steps: int = 500\n","    num_workers: int = 1\n","    \n","    # Logging configuration\n","    project_name: str = \"Test\"\n","    run_name: Optional[str] = \"test\"\n","    @classmethod\n","    def from_yaml(cls, yaml_path: str) -> 'TrainingConfig':\n","        with open(yaml_path, 'r') as f:\n","            config_dict = yaml.safe_load(f)\n","        return cls(**config_dict)\n","    \n","    def save_yaml(self, yaml_path: str):\n","        with open(yaml_path, 'w') as f:\n","            yaml.dump(self.__dict__, f)\n","\n","conf = TrainingConfig()\n","conf.save_yaml(\"config.yaml\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T17:19:37.473614Z","iopub.status.busy":"2025-01-28T17:19:37.473301Z","iopub.status.idle":"2025-01-28T17:20:31.131540Z","shell.execute_reply":"2025-01-28T17:20:31.130504Z","shell.execute_reply.started":"2025-01-28T17:19:37.473588Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-01-28 17:19:47.516005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-28 17:19:47.516526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-28 17:19:47.551927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-28 17:19:47.552639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-28 17:19:47.564311: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-28 17:19:47.564691: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Dataset({\n","    features: ['input_ids', 'attention_mask'],\n","    num_rows: 861102\n","})\n","Dataset({\n","    features: ['input_ids', 'attention_mask'],\n","    num_rows: 861102\n","})\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marthur-lagacherie\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marthur-lagacherie\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250128_172025-za9k6122\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/arthur-lagacherie/test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/arthur-lagacherie/test/runs/za9k6122\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250128_172025-vent5jis\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/arthur-lagacherie/test\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/arthur-lagacherie/test/runs/vent5jis\u001b[0m\n","Training:   0%|                                      | 0/861102 [00:00<?, ?it/s]\n","[rank0]: Traceback (most recent call last):\n","[rank0]:   File \"/kaggle/working/main.py\", line 184, in <module>\n","[rank0]:     main()\n","[rank0]:   File \"/kaggle/working/main.py\", line 181, in main\n","[rank0]:     train(config, mloramodel, train_dataloader, val_dataloader)\n","[rank0]:   File \"/kaggle/working/main.py\", line 138, in train\n","[rank0]:     outputs = model(**batch)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","[rank0]:     return self._call_impl(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","[rank0]:     return forward_call(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1643, in forward\n","[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1459, in _run_ddp_forward\n","[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","[rank0]:     return self._call_impl(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","[rank0]:     return forward_call(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 831, in forward\n","[rank0]:     outputs = self.model(\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","[rank0]:     return self._call_impl(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","[rank0]:     return forward_call(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 589, in forward\n","[rank0]:     layer_outputs = decoder_layer(\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","[rank0]:     return self._call_impl(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","[rank0]:     return forward_call(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 332, in forward\n","[rank0]:     hidden_states, self_attn_weights = self.self_attn(\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","[rank0]:     return self._call_impl(*args, **kwargs)\n","[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","[rank0]:     return forward_call(*args, **kwargs)\n","[rank0]:   File \"/kaggle/working/modeling.py\", line 179, in forward\n","[rank0]:     attn_weights = attn_weights_1 - lambda_full * attn_weights_2\n","[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 28.12 MiB is free. Process 5603 has 14.71 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 93.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","[rank0]:[W128 17:20:27.685848805 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","W0128 17:20:29.708000 242 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 252 closing signal SIGTERM\n","E0128 17:20:30.224000 242 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 251) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/accelerate\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n","    args.func(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1163, in launch_command\n","    multi_gpu_launcher(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 792, in multi_gpu_launcher\n","    distrib_run.run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","main.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2025-01-28_17:20:29\n","  host      : 5270821ec9e3\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 251)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n"]}],"source":["!accelerate launch main.py --config \"config.yaml\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30839,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
