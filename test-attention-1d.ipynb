{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-03T14:08:17.417640Z","iopub.execute_input":"2024-07-03T14:08:17.418246Z","iopub.status.idle":"2024-07-03T14:08:18.631127Z","shell.execute_reply.started":"2024-07-03T14:08:17.418186Z","shell.execute_reply":"2024-07-03T14:08:18.629713Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2024-07-03T14:08:18.633357Z","iopub.execute_input":"2024-07-03T14:08:18.633995Z","iopub.status.idle":"2024-07-03T14:08:22.855026Z","shell.execute_reply.started":"2024-07-03T14:08:18.633952Z","shell.execute_reply":"2024-07-03T14:08:22.853808Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n  \"\"\"one head of self-attention\"\"\"\n\n  def __init__(self, head_size):\n      super().__init__()\n      self.key = nn.Linear(n_embd, head_size, bias=False)\n      self.query = nn.Linear(n_embd, head_size, bias=False)\n      self.value = nn.Linear(n_embd, head_size, bias=False)\n      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n      self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    B,T,C = x.shape\n    k = self.key(x)\n    q = self.query(x)\n    # compute attention score (\"affinities\")\n    wei = q @ k.transpose(-2, -1) * C**-0.5\n    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n    wei = F.softmax(wei, dim=-1)# (B, T, T)\n    wei = self.dropout(wei)\n    # perform the weighted aggregation of the values\n    v = self.value(x)\n    out = wei @ v # (B, T, C)\n    return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_embd = 5\nmem_size = 7\n\ntoken = torch.randn(1, n_embd)\nm = torch.randn(1, mem_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T14:08:44.009323Z","iopub.execute_input":"2024-07-03T14:08:44.009892Z","iopub.status.idle":"2024-07-03T14:08:44.039347Z","shell.execute_reply.started":"2024-07-03T14:08:44.009851Z","shell.execute_reply":"2024-07-03T14:08:44.038140Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"key = nn.Linear(n_embd, n_embd, bias=False)\nquery = nn.Linear(mem_size, n_embd, bias=False)\nvalue = nn.Linear(mem_size, n_embd, bias=False)\ntril = torch.tril(torch.ones(n_embd, n_embd))","metadata":{"execution":{"iopub.status.busy":"2024-07-03T14:18:12.811222Z","iopub.execute_input":"2024-07-03T14:18:12.811612Z","iopub.status.idle":"2024-07-03T14:18:12.818232Z","shell.execute_reply.started":"2024-07-03T14:18:12.811581Z","shell.execute_reply":"2024-07-03T14:18:12.817304Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"k = key(token) # (1, n_embd)\nq = query(m) # (1, n_embd)\n\nwei = q.T @ k * n_embd**-0.5 # (n_embd, n_embd)\nwei = wei.masked_fill(tril[:n_embd, :n_embd] == 0, float('-inf')) # (n_embd, n_embd)\nwei = F.softmax(wei, dim=-1)# (n_embd, n_embd)\n\nv = value(m) # (1, n_embd)\nout = v @ wei # (1, n_embd)\nout","metadata":{"execution":{"iopub.status.busy":"2024-07-03T14:25:16.510114Z","iopub.execute_input":"2024-07-03T14:25:16.511081Z","iopub.status.idle":"2024-07-03T14:25:16.522559Z","shell.execute_reply.started":"2024-07-03T14:25:16.511046Z","shell.execute_reply":"2024-07-03T14:25:16.521313Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.0211, -0.3923, -0.0543, -0.1658, -0.0071]], grad_fn=<MmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, head_size, m_size):\n        super().__init__()\n        self.head_size = head_size\n        \n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(m_size, head_size, bias=False)\n        self.value = nn.Linear(m_size, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(head_size, head_size)))\n    \n    def forward(self, token, memory):\n        k = key(token) # (1, head_size)\n        q = query(memory) # (1, head_size)\n\n        wei = q.T @ k * self.head_size**-0.5 # (head_size, head_size)\n        wei = wei.masked_fill(tril[:self.head_size, :self.head_size] == 0, float('-inf')) # (head_size, head_size)\n        wei = F.softmax(wei, dim=-1)# (head_size, head_size)\n\n        v = value(memory) # (1, head_size)\n        out = v @ wei # (1, head_size)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-03T14:25:34.013979Z","iopub.execute_input":"2024-07-03T14:25:34.014738Z","iopub.status.idle":"2024-07-03T14:25:34.025781Z","shell.execute_reply.started":"2024-07-03T14:25:34.014701Z","shell.execute_reply":"2024-07-03T14:25:34.024380Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"multiple heads of self-attention in parallel\"\"\"\n\n    def __init__(self, num_heads, head_size, m_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, m_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(num_heads * head_size, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-03T14:25:34.330793Z","iopub.execute_input":"2024-07-03T14:25:34.331249Z","iopub.status.idle":"2024-07-03T14:25:34.340004Z","shell.execute_reply.started":"2024-07-03T14:25:34.331209Z","shell.execute_reply":"2024-07-03T14:25:34.338720Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}